# -*- coding: utf-8 -*-
"""Epsilon-Greedy_Softmax_UCB1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xt9W3i_bGW-2pVY2334hltADjbunzNLS
"""

""" EPSILON_GREEDY_Algorithm"""

# Importing the libraries
import random
import numpy as np
import matplotlib.pyplot as plt
import time

"""Setting up the 10 armed bandit testbed for 2000 bandits"""

bandits = 2000 #No of machines
arms = 10 #lever/action in each machine
runs = 1000 #No of pulls

# epsilon = [[0,'k'],[0.1,'r'],[0.2,'g'],[0.3,'b'],[1,'m']] #Different epsilon values
epsilon=[[0.1,'r']]
true_values = []
for k in range(bandits):
  tv = {}
  for i in range(arms):
    tv[f'arm{i}'] = np.random.randn() #Setting up mean values for reward distribution for each action in all bandits
  true_values.append(tv)


#ALGORITHM

for eps in epsilon:

  start=time.clock()
  totl_time=0.0
  
  Rewards_run_bandits = []
  for k in range(bandits):
    Rewards_run = []
    Reward_estimate = {}

    for i in range(arms):
        Reward_estimate[f'arm{i}'] = float(np.random.normal(true_values[k][f'arm{i}'], 1, 1)) #Estimate value of reward from their distribution
    a_star = np.argmax(list(Reward_estimate.values())) #Best action for a bandit

    count = {}
    for j in range(arms):
        count[f'arm{j}'] = 1 #Each arm of all bandits are selected for once at start
    for l in range(runs):
        a_star = np.argmax(list(Reward_estimate.values())) #Best action for a bandit for the particular run
        a = np.random.uniform() #No. taken from an uniform distribution of 0 to 1 randomly
        if a > eps[0]: #No. compared to the epsilon values
            action = f'arm{a_star}' #Exploitation ; Best arm till now is chosen
        else:
            action = random.choice(list(Reward_estimate.keys())) #Exploration ; Any arm is chosen randomly

        Imm_Reward = np.random.normal(true_values[k][action], 1) #Reward is generated based on chosen action from the reward distribution
        Rewards_run.append(Imm_Reward) #Saving the rewards of chosen action for a particular run for 1 bandit and finally 1000 runs for 1 bandit

        Reward_estimate[action] = (Reward_estimate[action]*count[action] + Imm_Reward)/(count[action]+1) #Updating the reward estimate
        count[action]+=1 #Stores the no of times one action is chosen

    Rewards_run_bandits.append(np.array(Rewards_run)) #Saving the rewards of 1000 runs of each bandit 
  Rewards_run_bandits = np.array(Rewards_run_bandits) #Converting to numpy array
  Average_Rewards = np.mean(Rewards_run_bandits, axis = 0) #Average of rewards of all 2000 bandits at all runs
  end = time.clock()
  totl_time = -start+end
  # print('Total_computing_time for ε = %f is %f' %(eps[0], totl_time))

Average_Rewards_epsilon_greedy = Average_Rewards



"""SOFTMAX_Algorithm"""

#Importing the librarires
import numpy as np
import matplotlib.pyplot as plt
import time


"""Setting up the 10 armed bandit testbed for 2000 bandits"""

bandits = 2000 #No of machines
arms = 10 #lever/action in each machine
runs = 1000 #No of pulls

# T = [[0.01,'r'],[0.02,'g'],[0.1,'b'],[1,'k']]
T=[[0.01,'r']]
true_values = np.random.randn(bandits, arms)

Average_Rewards = []

#ALGORITHM

for t in T:

  start=time.clock()
  totl_time=0.0

  Rewards_run_bandits = []
  for i in range(bandits): #Running the Softmax algorithm for 2000 bandit problems separately
    Reward_estimate = np.random.normal(true_values[i], 1) #Initialising the estimates values of 10 arms of each bandit in each run
    count = [1]*10 #Picking up each arm once



    Rewards_run = [] 
    for k in range(runs):

      Prob = np.array(list(map(np.exp, np.array(Reward_estimate)/t[0])))  #Applying the softmax function with Gibbs equation to arms
      Prob = Prob/np.sum(Prob)   #Calculating the probabilities
    
      action = int(np.random.choice(list(range(arms)), 1, p=Prob)) #Choosing the arm based on probability 
      Reward = np.random.normal(true_values[i][action], 1) #Estimate value of reward for the chosen action from respective distribution

      Reward_estimate[action] = (Reward_estimate[action]*count[action] + Reward)/(count[action]+1) # Updating the estimate of that action
      count[action]+=1
      Rewards_run.append(Reward) #Saving the rewards of chosen action for a particular run for 1 bandit and finally 1000 runs for 1 bandit
      
    Rewards_run_bandits.append(Rewards_run) #Saving the rewards of 1000 runs of each bandit 

  Rewards_run_bandits = np.array(Rewards_run_bandits) #Converting to numpy array
  Average_Rewards.append(np.mean(Rewards_run_bandits,axis = 0)) #Average of rewards of all 2000 bandits at all runs

  end = time.clock()
  totl_time = -start+end
  # print('Total_computing_time for t = %f is %f' %(t[0], totl_time))

Average_Rewards_softmax = Average_Rewards[0]



"""UCB1_ALGORITHM"""

#Importing libraries
import numpy as np
import matplotlib.pyplot as plt
import time

"""Setting up the 10 armed bandit testbed for 2000 bandits"""

bandits  = 2000 #No of machines
arms = 10 #lever/action in each machine
runs = 1000 #No of pulls

true_values = np.random.randn(bandits, arms) #Setting up mean values for reward distribution for each action in all bandits

Rewards_run_bandits = []
# c = [[0.1,'r'], [2,'g'], [5,'k']]
c=[[2,'g']] #Degree of Exploration
#ALGORITHM

for d in c:

  start=time.clock()
  totl_time=0.0

  for i in range(bandits):
    Rewards_run = []
    Reward_estimate = np.random.normal(true_values[i], 1) #Estimate value of reward from their distribution
    count = list(np.ones(arms)) #Selectiong all arms once at start
    bands = np.array(Reward_estimate[:]) + (d[0]*np.log(arms)/count[1])**0.5 #Updating the bands of all arms cosen once at start
    Rewards_run = Rewards_run + list(bands) 

    #Now, choosing best arms from the algorithm
    for k in range(arms, runs+arms):
      
      action = np.argmax(bands) #Best action
      Imm_Reward = np.random.normal(true_values[i][action], 1) #Reward of the best action
      Reward_estimate[action] = (Reward_estimate[action]*count[action] + Imm_Reward)/(count[action]+1) #Updating the estimate of reward
      count[action]+=1 #Updating the count of chosen action

      """ updating the bands for each run"""

      for l in range(arms):
        bands[l] = Reward_estimate[l] + (d[0]*np.log(k)/count[l])**0.5 #updating the bands according to chosen arms
      Rewards_run.append(Imm_Reward) #Saving the rewards of chosen action for a particular run for 1 bandit and finally 1000 runs for 1 bandit
    Rewards_run_bandits.append(Rewards_run) #Saving the rewards of 1000 runs of each bandit

  Average_Rewards = np.mean(Rewards_run_bandits, axis = 0)[arms:] #Average of rewards of all 2000 bandits at all runs
  
  # end = time.clock()
  # totl_time = -start+end
  # print('Total_computing_time for ddd = %f is %f' %(d[0], totl_time))

Average_Rewards_UCB1 = Average_Rewards


#PLOTTING THE GRAPH SHOWING COMPARISON BETWEEN DIFFERENT ALGORITHMS

import matplotlib.pyplot as plt

plt.plot(list(range(1000)), Average_Rewards_epsilon_greedy, c = 'k',label = f'Epsilon_Greedy for ε = 0.1' )
plt.plot(list(range(1000)), Average_Rewards_softmax, c = 'b', label = f'Softmax for t = 0.01')
plt.plot(list(range(1000)), Average_Rewards_UCB1, c = 'r', label = f'UCB1 for c = 2')
plt.legend()
plt.title('Comparison between different Algorithms')
plt.xlabel('Runs')
plt.ylabel('Average Reward')

